{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end AI Infrastructure Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to take the place of the steps of the eventual Airflow DAGs for the purpose of validating our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Setup - Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Builout - Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part should be one click. Just run \"infrastructure/deployment/deploy-infrastructure.sh -s \\<subscription_id\\>\" and all of the necessary Azure resources should be built out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Configuration - Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are Azure specific. They will handle setting up the credentials/configurations required for the rest of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.mgmt.containerservice import ContainerServiceClient\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "from azure.mgmt.resource import SubscriptionClient\n",
    "import json\n",
    "\n",
    "# Read in configuration\n",
    "azure_config = json.load(open('config.json'))\n",
    "subscription_id = azure_config[\"subscription_id\"]\n",
    "resource_group_name = azure_config[\"main_resource_group\"]\n",
    "cluster_names = azure_config[\"cluster_names\"]\n",
    "\n",
    "# Authenticate to Azure\n",
    "default_credential = DefaultAzureCredential()\n",
    "subscription_client = SubscriptionClient(credential=default_credential)\n",
    "datalake_service_client = DataLakeServiceClient(azure_config[\"account_url\"], credential=default_credential)\n",
    "storage_client = StorageManagementClient(credential=default_credential, subscription_id=subscription_id)\n",
    "\n",
    "# Find the subscription ID\n",
    "for subscription in subscription_client.subscriptions.list():\n",
    "    if subscription.subscription_id == subscription_id:\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Subscription not found\")\n",
    "\n",
    "# Get the AKS cluster client\n",
    "container_service_client = ContainerServiceClient(default_credential, subscription_id)\n",
    "\n",
    "# Get and save kubeconfigs to ./manual_pipeline_files/cluster_configs/<cluster_name>.yaml\n",
    "for cluster_name in cluster_names:\n",
    "    # Get the kubeconfig file for the cluster\n",
    "    credential_results = container_service_client.managed_clusters.list_cluster_user_credentials(\n",
    "        resource_group_name,\n",
    "        cluster_name,\n",
    "    )\n",
    "\n",
    "    # The kubeconfig file is in base64, so we need to decode it\n",
    "    kubeconfig = credential_results.kubeconfigs[0].value.decode(\"utf-8\")\n",
    "\n",
    "    # Save the kubeconfig to a file\n",
    "    with open(f'./manual_pipeline_files/cluster_configs/{cluster_name}.yaml', 'w') as file:\n",
    "        file.write(kubeconfig)\n",
    "\n",
    "# Get the storage account key\n",
    "storage_account_key = storage_client.storage_accounts.list_keys(resource_group_name, azure_config[\"account_name\"]).keys[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be structuring and storing data within our SQL database. This will be the first step in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Create a spark-data filesystem with a spark-upload directory\n",
    "# Create the file system if it does not exist\n",
    "try:\n",
    "    datalake_service_client.create_file_system(file_system=\"spark-data\")\n",
    "except Exception as e:\n",
    "    if e.__class__.__name__ == \"ResourceExistsError\":\n",
    "        print(\"Resource already exists\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Create the spark-upload directory if it does not exist\n",
    "try:\n",
    "    datalake_service_client.get_file_system_client(file_system=\"spark-data\").create_directory(\"spark-jobs\")\n",
    "except Exception as e:\n",
    "    if e.__class__.__name__ == \"ResourceExistsError\":\n",
    "        print(\"Resource already exists\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab spark-aks main k8s server e.x. https://dnsprefix-ww13q3en.hcp.eastus.azmk8s.io:443\n",
    "spark_cluster_name = [cluster for cluster in cluster_names if \"spark\" in cluster][0]\n",
    "main_server = container_service_client.managed_clusters.get(resource_group_name, spark_cluster_name).fqdn\n",
    "\n",
    "image_repo = azure_config[\"image_repo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['cp', '-r', '../jobs/.', './manual_pipeline_files/spark_jobs/'], returncode=0)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the spark job to manual_pipeline_files\n",
    "os.makedirs(\"./manual_pipeline_files/spark_jobs\", exist_ok=True)\n",
    "subprocess.run([\"cp\", \"-r\", \"../jobs/.\", \"./manual_pipeline_files/spark_jobs/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_server = \"https://127.0.0.1:54565\"\n",
    "account_name = azure_config[\"account_name\"]\n",
    "data_filesystem = azure_config[\"data_filesystem\"]\n",
    "storage_account_key = \"your_storage_account_key\"\n",
    "tenant_id = azure_config[\"spark_spn\"][\"tenant_id\"]\n",
    "client_id = azure_config[\"spark_spn\"][\"client_id\"]\n",
    "sql_serverurl = azure_config[\"sql-db\"][\"server\"]\n",
    "sql_port = azure_config[\"sql-db\"][\"port\"]\n",
    "sql_database = azure_config[\"sql-db\"][\"database\"]\n",
    "sql_username = azure_config[\"sql-db\"][\"username\"]\n",
    "sql_password = azure_config[\"sql-db\"][\"password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit \\\n",
    "  --master k8s://{main_server} \\\n",
    "  --deploy-mode cluster \\\n",
    "  --name spark-dist-training \\\n",
    "  --conf spark.kubernetes.container.image={image_repo}/pyspark-test:latest \\\n",
    "  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n",
    "  --conf spark.executor.instances=1 \\\n",
    "  --conf spark.kubernetes.namespace=default \\\n",
    "  --conf \"spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n",
    "  --conf spark.kubernetes.trust.certificates=true \\\n",
    "  --conf spark.driver.port=7070 \\\n",
    "  --conf spark.driver.host=spark-driver-headless.default.svc.cluster.local \\\n",
    "  --conf spark.ui.port=7069 \\\n",
    "  --conf \"vars.account.name={account_name}\" \\\n",
    "  --conf \"vars.account.datafilesystem={data_filesystem}\" \\\n",
    "  --conf spark.kubernetes.executor.label.azure.workload.identity/use=true \\\n",
    "  --conf spark.kubernetes.executor.label.aadpodidbinding=vmscalesetidentity \\\n",
    "  --conf spark.kubernetes.driver.label.aadpodidbinding=vmscalesetidentity \\\n",
    "  --conf spark.kubernetes.driver.label.azure.workload.identity/use=true \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.key.{account_name}.dfs.core.windows.net={storage_account_key}\" \\\n",
    "  --conf \"spark.kubernetes.file.upload.path=abfss://spark-data@{account_name}.dfs.core.windows.net/spark-jobs\" \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.oauth2.msi.tenant={tenant_id}\" \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.oauth2.client.id={client_id}\" \\\n",
    "  --conf \"vars.sql.serverurl={sql_serverurl}\" \\\n",
    "  --conf \"vars.sql.port={sql_port}\" \\\n",
    "  --conf \"vars.sql.database={sql_database}\" \\\n",
    "  --conf \"vars.sql.username={sql_username}\" \\\n",
    "  --conf \"vars.sql.password={sql_password}\" \\\n",
    "  --conf \"spark.jars.packages=org.apache.hadoop:hadoop-azure:3.3.4,com.azure:azure-storage-blob:12.25.1,com.microsoft.sqlserver:mssql-jdbc:12.6.1.jre11\" \\\n",
    "  \"manual_pipeline_files/spark_jobs/data-collection/spark-sklearn-iris-collection.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing - Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "driver_host=\"spark-driver-headless.default.svc.cluster.local\"\n",
    "account_name = azure_config[\"account_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit \\\n",
    "  --master k8s://{main_server} \\\n",
    "  --deploy-mode cluster \\\n",
    "  --name spark-dist-training \\\n",
    "  --conf spark.kubernetes.container.image={image_repo}/pyspark-test:latest \\\n",
    "  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n",
    "  --conf spark.executor.instances=1 \\\n",
    "  --conf spark.kubernetes.namespace=default \\\n",
    "  --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n",
    "  --conf spark.kubernetes.trust.certificates=true \\\n",
    "  --conf spark.driver.port=7070 \\\n",
    "  --conf spark.driver.host=spark-driver-headless.default.svc.cluster.local \\\n",
    "  --conf spark.ui.port=7069 \\\n",
    "  --conf \"vars.account.name={account_name}\" \\\n",
    "  --conf \"vars.account.datafilesystem={data_filesystem}\" \\\n",
    "  --conf spark.kubernetes.executor.label.azure.workload.identity/use=true \\\n",
    "  --conf spark.kubernetes.executor.label.aadpodidbinding=vmscalesetidentity \\\n",
    "  --conf spark.kubernetes.driver.label.aadpodidbinding=vmscalesetidentity \\\n",
    "  --conf spark.kubernetes.driver.label.azure.workload.identity/use=true \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.key.{account_name}.dfs.core.windows.net={storage_account_key}\" \\\n",
    "  --conf \"spark.kubernetes.file.upload.path=abfss://spark-data@{account_name}.dfs.core.windows.net/spark-jobs\" \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.oauth2.msi.tenant={tenant_id}\" \\\n",
    "  --conf \"spark.hadoop.fs.azure.account.oauth2.client.id={client_id}\" \\\n",
    "  --conf \"vars.sql.serverurl={sql_serverurl}\" \\\n",
    "  --conf \"vars.sql.port={sql_port}\" \\\n",
    "  --conf \"vars.sql.database={sql_database}\" \\\n",
    "  --conf \"vars.sql.username={sql_username}\" \\\n",
    "  --conf \"vars.sql.password={sql_password}\" \\\n",
    "  --conf spark.jars.packages=\"org.apache.hadoop:hadoop-azure:3.3.4,com.azure:azure-storage-blob:12.25.1\" \\\n",
    "    \"manual_pipeline_files/iris_spark_job.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push the docker image at ./manual_pipeline_files/Dockerfile.pretraining\n",
    "# This docker image will be submitted to a kubeflow pytorch training operator in the next step\n",
    "# The tag should be generated here, random or based on the git commit hash\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import string\n",
    "\n",
    "def random_string(string_length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(string_length))\n",
    "\n",
    "image_name = azure_config[\"iris_torch_job_name\"]\n",
    "tag = random_string()\n",
    "full_image_name = f\"{image_repo}/{image_name}:{tag}\"\n",
    "\n",
    "subprocess.run([\"cp\", \"-r\", \"../models/iris\", \"./manual_pipeline_files/models\"])\n",
    "os.chdir(\"manual_pipeline_files\")\n",
    "subprocess.run([\"docker\", \"build\", \"-t\", f\"{full_image_name}\", \"-f\", \"Dockerfile.pretraining\", \".\"])\n",
    "subprocess.run([\"docker\", \"push\", f\"{full_image_name}\"])\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - TorchJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to open and replace the image tag with the one we just built\n",
    "import yaml\n",
    "\n",
    "with open(\"../pipeline/kubeflow/pytorchjobs/torchjob-iris.yaml\", \"r\") as f:\n",
    "    torchjob_yaml = yaml.safe_load(f)\n",
    "    \n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Master\"][\"template\"][\"spec\"][\"containers\"][0][\"image\"] = f\"{full_image_name}\"\n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Worker\"][\"template\"][\"spec\"][\"containers\"][0][\"image\"] = f\"{full_image_name}\"\n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Master\"][\"template\"][\"spec\"][\"containers\"][0][\"args\"] = [\"--learning_rate\", \"0.001\", \"--account_name\", account_name]\n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Worker\"][\"template\"][\"spec\"][\"containers\"][0][\"args\"] = [\"--learning_rate\", \"0.001\", \"--account_name\", account_name]\n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Master\"][\"template\"][\"metadata\"][\"labels\"][\"aadpodidbinding\"] = \"vmscalesetidentity\"\n",
    "torchjob_yaml[\"spec\"][\"pytorchReplicaSpecs\"][\"Worker\"][\"template\"][\"metadata\"][\"labels\"][\"aadpodidbinding\"] = \"vmscalesetidentity\"\n",
    "      \n",
    "# We'll save our new yaml at ./manual_pipeline_files/torchjob-iris.yaml\n",
    "with open(\"./manual_pipeline_files/torchjob-iris.yaml\", \"w\") as f:\n",
    "    yaml.dump(torchjob_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll apply the yaml to the training kubeflow cluster. Switch to the training cluster based on the config file\n",
    "training_cluster_name = [cluster for cluster in cluster_names if \"train\" in cluster][0]\n",
    "\n",
    "# # Load kubeconfig from ./manual_pipeline_files/cluster_configs/<training_cluster_name>.yaml\n",
    "os.environ[\"KUBECONFIG\"] = f\"./manual_pipeline_files/cluster_configs/{training_cluster_name}.yaml\"\n",
    "\n",
    "# Apply the yaml\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"./manual_pipeline_files/torchjob-iris.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll the status of the job until it's done. We can use the kubectl command to dump the status of the job\n",
    "# Then we can parse the output to see if the job is done\n",
    "import time\n",
    "import json\n",
    "\n",
    "max_time = 60 * 60 # 1 hour\n",
    "start_time = time.time()    \n",
    "while time.time() - start_time < max_time:\n",
    "    status = subprocess.run([\"kubectl\", \"get\", \"pytorchjobs\", \"iris-torch-job\", \"-o\", \"json\", \"-n\", \"kubeflow\"], capture_output=True)\n",
    "    status = json.loads(status.stdout)\n",
    "    if status[\"status\"][\"conditions\"][-1][\"type\"] == \"Succeeded\":\n",
    "        print(\"Job succeeded\")\n",
    "        break\n",
    "    elif status[\"status\"][\"conditions\"][-1][\"type\"] == \"Failed\":\n",
    "        print(\"Job failed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, input_size * 2)\n",
    "        self.fc2 = torch.nn.Linear(input_size * 2, input_size * 2)\n",
    "        self.fc3 = torch.nn.Linear(input_size * 2, input_size)\n",
    "        self.fc4 = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class IrisDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, target_column: str):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data.loc[idx, self.data.columns != self.target_column]\n",
    "        target = self.data.loc[idx, self.data.columns == self.target_column]\n",
    "\n",
    "        features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        target = torch.tensor(target.values[0], dtype=torch.long)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Torch model weights are saved at \"https://<file_system>@<account_name>.dfs.core.windows.net/iris-models/weights/iris_model_weights.pt\"\n",
    "# We can download the weights to the local filesystem\n",
    "# We can use the datalake_service_client to download the weights\n",
    "\n",
    "# Get the filesystem client\n",
    "filesystem_client = datalake_service_client.get_file_system_client(azure_config[\"model_filesystem\"])\n",
    "\n",
    "# Download the weights\n",
    "file_client = filesystem_client.get_file_client(\"iris_models/weights/iris_model_weights.pt\")\n",
    "with open(\"./manual_pipeline_files/iris_model_weights.pt\", \"wb\") as file:\n",
    "    weights = file_client.download_file()\n",
    "    file.write(weights.readall())\n",
    "\n",
    "model = Classifier(4, 3)\n",
    "model.load_state_dict(torch.load(\"./manual_pipeline_files/iris_model_weights.pt\"))\n",
    "\n",
    "# We can now use the model to make predictions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = sklearn.datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df[\"target\"] = iris.target\n",
    "\n",
    "# Convert the dataframe to the IrisDataset class\n",
    "iris_dataset = IrisDataset(iris_df, \"target\")\n",
    "\n",
    "# Run a for loop to and track the accuracy of the model\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(iris_dataset)):\n",
    "    features, target = iris_dataset[i]\n",
    "    output = model(features)\n",
    "    prediction = np.argmax(output.detach().numpy())\n",
    "    if prediction == target:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment - Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the model is good enough, we build the torchserve docker image with the file being at ../models/iris/serving/Dockerfile\n",
    "# We need to pull the .mar file from the datalake and put it at ../models/iris/training/iris_model.mar\n",
    "\n",
    "file_client = filesystem_client.get_file_client(\"iris_models/mar/iris_model.mar\")\n",
    "with open(\"../models/iris/serving/iris_model.mar\", \"wb\") as file:\n",
    "    mar = file_client.download_file()\n",
    "    file.write(mar.readall())\n",
    "\n",
    "full_serve_image_name = f\"{image_repo}/{azure_config['iris_torchserve_name']}\"\n",
    "\n",
    "# Grab current working directory before changing it\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(\"../models/iris/serving\")\n",
    "subprocess.run([\"docker\", \"build\", \"-t\", f\"{full_serve_image_name}\", \".\"])\n",
    "subprocess.run([\"docker\", \"push\", f\"{full_serve_image_name}\"])\n",
    "os.chdir(current_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment - TorchServe/KFServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now deploy the torchserve model to the serving cluster\n",
    "# We'll use the kserve-iris.yaml file at ../pipeline/kubeflow/kserve/kserve-iris.yaml\n",
    "with open(\"../pipeline/kubeflow/kserve/kserve-iris.yaml\", \"r\") as f:\n",
    "    kserve_yaml = yaml.safe_load(f)\n",
    "\n",
    "account_name = azure_config[\"account_name\"]\n",
    "file_system_name = azure_config[\"model_filesystem\"]\n",
    "storage_uri = f\"https://{account_name}.dfs.core.windows.net/{file_system_name}/iris_models/mar\"\n",
    "kserve_yaml[\"spec\"][\"predictor\"][\"model\"][\"storageUri\"] = storage_uri\n",
    "\n",
    "# We'll save our new yaml at ./manual_pipeline_files/kserve-iris.yaml\n",
    "with open(\"./manual_pipeline_files/kserve-iris.yaml\", \"w\") as f:\n",
    "    yaml.dump(kserve_yaml, f)\n",
    "    \n",
    "# In kserve-secret.yaml, we need to pull the credential info from azure_config[\"kf_serve_spn\"] and add them to the yaml.\n",
    "with open(\"../pipeline/kubeflow/kserve/kserve-secret.yaml\", \"r\") as f:\n",
    "    kserve_secret_yaml = yaml.safe_load(f)\n",
    "\n",
    "kserve_secret_yaml[\"stringData\"][\"AZ_CLIENT_ID\"] = azure_config[\"kf_serve_spn\"][\"client_id\"]\n",
    "kserve_secret_yaml[\"stringData\"][\"AZ_CLIENT_SECRET\"] = azure_config[\"kf_serve_spn\"][\"secret\"]\n",
    "kserve_secret_yaml[\"stringData\"][\"AZ_SUBSCRIPTION_ID\"] = azure_config[\"subscription_id\"]\n",
    "kserve_secret_yaml[\"stringData\"][\"AZ_TENANT_ID\"] = azure_config[\"kf_serve_spn\"][\"tenant_id\"]\n",
    "\n",
    "# We'll save our new yaml at ./manual_pipeline_files/kserve-secret.yaml\n",
    "with open(\"./manual_pipeline_files/kserve-secret.yaml\", \"w\") as f:\n",
    "    yaml.dump(kserve_secret_yaml, f)\n",
    "\n",
    "# We'll apply the yaml to the serving kubeflow cluster. Switch to the serving cluster based on the config file\n",
    "serving_cluster_name = [cluster for cluster in cluster_names if \"serv\" in cluster][0]\n",
    "\n",
    "# Standard Cluster Load\n",
    "# Load kubeconfig from ./manual_pipeline_files/cluster_configs/<serving_cluster_name>.yaml\n",
    "os.environ[\"KUBECONFIG\"] = f\"./manual_pipeline_files/cluster_configs/{serving_cluster_name}.yaml\"\n",
    "\n",
    "# Local Cluster Load\n",
    "# os.environ[\"KUBECONFIG\"] = f\"/home/{os.environ['USER']}/.kube/config\" # Now switch context to minikube for testing\n",
    "# subprocess.run([\"kubectl\", \"config\", \"use-context\", \"minikube\"])\n",
    "\n",
    "# Apply the yaml\n",
    "generate_kserve_secret = subprocess.run([\"kubectl\", \"apply\", \"-f\", \"./manual_pipeline_files/kserve-secret.yaml\"], capture_output=True)\n",
    "generate_kserve_secret.check_returncode()\n",
    "print(generate_kserve_secret.stdout.decode(\"utf-8\"))\n",
    "\n",
    "deploy_inference_service = subprocess.run([\"kubectl\", \"apply\", \"-f\", \"./manual_pipeline_files/kserve-iris.yaml\"], capture_output=True)\n",
    "deploy_inference_service.check_returncode()\n",
    "print(deploy_inference_service.stdout.decode(\"utf-8\")) \n",
    "\n",
    "# Delete the modified yaml files\n",
    "os.remove(\"./manual_pipeline_files/kserve-iris.yaml\")\n",
    "os.remove(\"./manual_pipeline_files/kserve-secret.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
